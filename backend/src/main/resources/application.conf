# The details of the actor system
actor-system.name = "backend"

# The SMTP settings for the application
smtp = {
  #host = "smtp.office365.com"
  #port = 587
  #username = "no-reply-soc@iac.com"
  #password = ${?SMTP_PASSWORD}

  host = "email-smtp.us-east-1.amazonaws.com"
  host = ${?SMTP_HOST}
  port = 587
  port = ${?SMTP_PORT}
  username = "noreply"
  username = ${?SMTP_USERNAME}
  password = ""
  password = ${?SMTP_PASSWORD}
  from_email = "noreply@soc.iac.com"
  from_email = ${?SMTP_FROM_EMAIL}

}

client = {
  url = "http://172.21.32.107"
  url = ${?CLIENT_URL}
}

rules = {
  max_retry_count = 3
  max_retry_count = ${?MAX_RETRY_COUNT}
  retry_thread_sleep_time = 10000 // time in millisecond
  retry_thread_sleep_time = ${?RETRY_THREAD_TIME}
}

encryption = {
  key = "jMhKlOuJnM34G6NHkqo9V010GhLAqOpF0BePojHgh1HgNg8^72k"
  key = ${?ENCRYPTION_KEY}
}

logs = {
  parallel-batch-logs = 10000
  parallel-batch-logs = ${?PARALLEL_BATCH_LOGS}
}

# The kudu datawarehouse configuration
kudu = {
  masters = "172.21.31.119:7051"
  masters = ${?KUDU_MASTERS}
  table = "logs"
}

api-dispatcher {
  # Dispatcher is the name of the event-based dispatcher
  type = Dispatcher
  # What kind of ExecutionService to use
  executor = "thread-pool-executor"
  # Configuration for the fork join pool
  #fork-join-executor {
    # Min number of threads to cap factor-based parallelism number to
   # parallelism-min = 2
    #parallelism-min =  ${?API_DISPATCHER_MIN_PARALLELISM}
    # Parallelism (threads) ... ceil(available processors * factor)
    #parallelism-factor = 2.0
    #parallelism-factor = ${?API_DISPATCHER_PARALLELISM_FACTOR}
    # Max number of threads to cap factor-based parallelism number to
    #parallelism-max = 10
    #parallelism-max =  ${?API_DISPATCHER_MAX_PARALLELISM}
  #}

  thread-pool-executor {
    fixed-pool-size = 16
    fixed-pool-size = ${?API_DISPATCHER_THREAD_POOL_SIZE}
  }

  # Throughput defines the maximum number of messages to be
  # processed per actor before the thread jumps to the next actor.
  # Set to 1 for as fair as possible.
  throughput = 1
  throughput =  ${?API_DISPATCHER_THROUGHPUT}
}

# Akka configuration for the backend system
akka {

  # Akka logging configuration
  loglevel = DEBUG
  loggers = ["akka.event.slf4j.Slf4jLogger"]

  # Akka actor configuration
  actor {

    # Actor provider configuration
    provider = "akka.cluster.ClusterActorRefProvider"

    # Available serializers and their keys
    serializers {
      proto = "akka.remote.serialization.ProtobufSerializer"
    }

    # Serializations to be applied for specified objects
    serialization-bindings {
      "scalapb.GeneratedMessage" = proto
    }

    # Akka router deployment configuration
    deployment {

      # Backend pipeline normalization router configuration
      /backend/pipeline/normalization/router {
        //router = round-robin-pool
        //cluster {
        // enabled = on
        //max-nr-of-instances-per-node = 100
        //max-nr-of-instances-per-node = ${?MAX_NORMALIZATION_INSTANCES_PER_NODE}
        //allow-local-routees = on
        //use-roles = ["worker"]
        //}
        router = random-pool
        nr-of-instances = 5
        nr-of-instances = ${?NORMALIZATION_INSTANCES}
      }

      # Backend pipeline enrichment router configuration
      /backend/pipeline/enrichment/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 3
          max-nr-of-instances-per-node = ${?MAX_ENRICHMENT_INSTANCES_PER_NODE}
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

      # Backend notification router configuration
      /backend/notification/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 3
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

      # Backend Kafka Producer router configuration
      /backend/pipeline/producer/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 3
          max-nr-of-instances-per-node = ${?MAX_PRODUCER_INSTANCES_PER_NODE}
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

      # Backend Kafka Consumer router configuration
      /backend/pipeline/consumer/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 10
          max-nr-of-instances-per-node = ${?MAX_CONSUMER_INSTANCES_PER_NODE}
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

      # Backend kudu Writer router configuration
      /backend/pipeline/store-manager/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 100
          max-nr-of-instances-per-node = ${?MAX_STORE_MANAGER_INSTANCES_PER_NODE}
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

      # Backend kudu Writer router configuration
      /backend/pipeline/log-producer/router {
        router = round-robin-pool
        cluster {
          enabled = on
          max-nr-of-instances-per-node = 3
          max-nr-of-instances-per-node = ${?MAX_LOG_PRODUCER_INSTANCES_PER_NODE}
          allow-local-routees = on
          use-roles = ["worker"]
        }
      }

    }

  }

  # Akka remoting configuration
  remote {
    enabled-transports = ["akka.remote.netty.tcp"]
    log-remote-lifecycle-events = off
    netty.tcp {
      hostname = "0.0.0.0"
      hostname = ${?AKKA_CLUSTER_HOST}
      port = 2551
      port = ${?AKKA_CLUSTER_PORT}

    }
    # If this is "on", Akka will log all outbound messages at
    # DEBUG level, if off then they are not logged
    log-sent-messages = on
    # If this is "on," Akka will log all inbound messages at
    # DEBUG level, if off then they are not logged
    log-received-messages = on
  }

  cluster-dispatcher {
    type = "Dispatcher"
    executor = "fork-join-executor"
    fork-join-executor {
      parallelism-min = 2
      parallelism-max = 4
    }
  }

  # Akka cluster configuration
  cluster {
    seed-nodes = []
    seed-nodes = ${?SEED_NODES}
    roles = ["seed", "worker"]
    role {
      seed.min-nr-of-members = 1
    }
    sharding.state-store-mode = ddata
    sharding.remember-entities = on
    sharding.distributed-data.durable.keys = []
    sharding.passivate-idle-entity-after = off
    downing-provider-class = "tanukki.akka.cluster.autodown.MajorityLeaderAutoDowning"
    use-dispatcher = akka.cluster-dispatcher
    failure-detector {
      heartbeat-interval = 1 s
      threshold = 15.0
      min-std-deviation = 200 ms
      acceptable-heartbeat-pause = 300 s
      expected-response-after = 300 s
      monitored-by-nr-of-members = 3
    }
  }

  management {
    http {
      hostname = "127.0.0.1"
      hostname = ${?AKKA_MANAGEMENT_HOSTNAME}
      bind-hostname = "0.0.0.0"
      port = 8558
      port = ${?AKKA_MANAGEMENT_PORT}
      bind-port = 8558
      bind-port = ${?AKKA_MANAGEMENT_PORT}
    }
    cluster.bootstrap {
      contact-point-discovery {
        required-contact-point-nr = 2
        required-contact-point-nr = ${?REQUIRED_CONTACT_POINTS}
        interval = 30 seconds
        exponential-backoff-max = 600 seconds
      }
    }
  }

  discovery {
    method = kubernetes-api
    method = ${?DISCOVERY_METHOD}
    kubernetes-api {
      pod-namespace = "default" // in which namespace cluster is running
      pod-namespace = ${?K8S_NAMESPACE}
      pod-label-selector = "app=akka-simple-cluster" // selector - how to find other cluster nodes
      pod-label-selector = ${?K8S_SELECTOR}
      pod-port-name = "management" // name of cluster management port
      pod-port-name = ${?K8S_MANAGEMENT_PORT}
    }
  }

  # Auto Downing Properties
  custom-downing {
    stable-after = 10000s
    majority-leader-auto-downing {
      majority-member-role = ""
      down-if-in-minority = true
      shutdown-actor-system-on-resolution = true
    }
  }
  # Akka quartz scheduler configuration
  quartz {
    threadPool.threadCount = 10
  }

  http {

    port = 8084
    port = ${?PORT}

    server {

      request-timeout = 60 s

    }

    host-connection-pool {

      # The maximum number of parallel connections that a connection pool to a
      # single host endpoint is allowed to establish. Must be greater than zero.
      max-connections = 1024

    }


  }

  # Properties for akka.kafka.ProducerSettings can be
  # defined in this section or a configuration section with
  # the same layout.
  kafka.producer {

    # Tuning parameter of how many sends that can run in parallel.
    parallelism = 100

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 60s

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the producer stages. Some blocking may occur.
    # When this value is empty, the dispatcher configured for the stream
    # will be used.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # The time interval to commit a transaction when using the `Transactional.sink` or `Transactional.flow`
    # for exactly-once-semantics processing.
    eos-commit-interval = 100ms

    # Properties defined by org.apache.kafka.clients.producer.ProducerConfig
    # can be defined in this configuration section.
    kafka-clients {

    }

    bootstrap-servers = "172.21.32.107:9092"
    bootstrap-servers = ${?BOOTSTARP_SERVERS}

    raw-log-topic = "raw-logs"
    raw-log-topic = ${?RAW_LOG_TOPIC}

    log-topic = "logs"
    log-topic = ${?LOG_TOPIC}

  }


  kafka.consumer {
    # Tuning property of scheduled polls.
    # Controls the interval from one scheduled poll to the next.
    poll-interval = 5s

    # Tuning property of the `KafkaConsumer.poll` parameter.
    # Note that non-zero value means that the thread that
    # is executing the stage will be blocked. See also the `wakup-timeout` setting below.
    poll-timeout = 50ms

    # The stage will delay stopping the internal actor to allow processing of
    # messages already in the stream (required for successful committing).
    # Prefer use of `DrainingControl` over a large stop-timeout.
    stop-timeout = 30s

    # Duration to wait for `KafkaConsumer.close` to finish.
    close-timeout = 20s

    # If offset commit requests are not completed within this timeout
    # the returned Future is completed `CommitTimeoutException`.
    # The `Transactional.source` waits this ammount of time for the producer to mark messages as not
    # being in flight anymore as well as waiting for messages to drain, when rebalance is triggered.
    commit-timeout = 15s

    # If commits take longer than this time a warning is logged
    commit-time-warning = 1s

    # Not used anymore (since 1.0-RC1)
    # wakeup-timeout = 3s

    # Not used anymore (since 1.0-RC1)
    # max-wakeups = 10

    # If set to a finite duration, the consumer will re-send the last committed offsets periodically
    # for all assigned partitions. See https://issues.apache.org/jira/browse/KAFKA-4682.
    commit-refresh-interval = infinite

    # Not used anymore (since 1.0-RC1)
    # wakeup-debug = true

    # Fully qualified config path which holds the dispatcher configuration
    # to be used by the KafkaConsumerActor. Some blocking may occur.
    use-dispatcher = "akka.kafka.default-dispatcher"

    # Properties defined by org.apache.kafka.clients.consumer.ConsumerConfig
    # can be defined in this configuration section.
    kafka-clients {
      # Disable auto-commit by default
      enable.auto.commit = false
    }

    # Time to wait for pending requests when a partition is closed
    wait-close-partition = 500ms

    # Limits the query to Kafka for a topic's position
    position-timeout = 5s

    # When using `AssignmentOffsetsForTimes` subscriptions: timeout for the
    # call to Kafka's API
    offset-for-times-timeout = 5s

    # Timeout for akka.kafka.Metadata requests
    # This value is used instead of Kafka's default from `default.api.timeout.ms`
    # which is 1 minute.
    metadata-request-timeout = 5s

    # Interval for checking that transaction was completed before closing the consumer.
    # Used in the transactional flow for exactly-once-semantics processing.
    eos-draining-check-interval = 30ms

    bootstrap-servers = "172.21.32.107:9092"
    bootstrap-servers = ${?BOOTSTARP_SERVERS}

    raw-log-consumer-group = "soc-backend-consumer-8"
    raw-log-consumer-group = ${?RAW_LOG_CONSUMER_GROUP}

    log-consumer-group = "group2"
    log-consumer-group = ${?LOG_CONSUMER_GROUP}

    auto-offset-reset-config = "latest"
    auto-offset-reset-config = ${?AUTO_OFFSET_RESET_CONFIG}

    log-topic = "logs"
    log-topic = ${?LOG_TOPIC}

    raw-log-topic = "raw-logs5"
    raw-log-topic = ${?RAW_LOG_TOPIC}

    stage-1-group-count = 10
    stage-1-group-count = ${?STAGE_1_GROUP_COUNT}

    stage-1-parallelism = 10
    stage-1-parallelism = ${?STAGE_1_PARALLELISM}

    # Camel kafka consumer settings
    camel {

      number-of-consumers = 10
      number-of-consumers = ${?CAMEL_KAFKA_NUMBER_OF_CONSUMERS}

      max-poll-records = 5
      max-poll-records = ${?CAMEL_KAFKA_MAX_POLL_RECORDS}

      aggregation-group-size = 5
      aggregation-group-size = ${?CAMEL_KAFKA_AGGREGATION_GROUP_SIZE}

      auto-offset-reset = "earliest"
      auto-offset-reset = ${?CAMEL_KAFKA_AUTO_OFFSET_RESET}

      heartbeat-interval-ms = 3000
      heartbeat-interval-ms = ${?CAMEL_KAFKA_HEARTBEAT_INTERVAL_MS}

      session-timeout-ms = 60000
      session-timeout-ms = ${?CAMEL_KAFKA_SESSION_TIMEOUT_MS}

      max-poll-interval-ms = 2147483647
      max-poll-interval-ms = ${?CAMEL_KAFKA_MAX_POLL_INTERVAL_MS}
    }

  }

  coordinated-shutdown {

    exit-jvm = on

  }

}

# Keycloak configurations
keycloak {

  config_path = "/keycloak.json"
  config_path = ${?KEYCLOAK_CONFIG_PATH}

  realm = "iac-soc"
  realm = ${?KEYCLOAK_REALM}

  #auth_server_host = "https://dev-soc.iac.com/auth"
  auth_server_host = "http://172.21.32.107:8082/auth"
  auth_server_host = ${?KEYCLOAK_HOST}

  auth_server_url = ${keycloak.auth_server_host}

  ssl_required = "external"

  resource = "iac-soc-frontend"
  resource = ${?KEYCLOAK_RESOURCE}

  public_client = true

  confidential_port = 0

  backend_resource = "admin-cli"
  backend_resource = ${?KEYCLOAK_BACKEND_RESOURCE}

  admin_username = "admin"
  admin_username = ${?KEYCLOAK_ADMIN_USERNAME}

  admin_password = "123456"
  admin_password = ${?KEYCLOAK_ADMIN_PASSWORD}


}
db {

  default
    {

      host = "172.21.32.107:3333"
      host = ${?MYSQL_HOST}

      driver = "com.mysql.jdbc.Driver"

      database = "iac_dev"
      database = ${?MYSQL_DATABASE}

      url = "jdbc:mysql://"${db.default.host}"/"${db.default.database}"?useSSL=false&allowPublicKeyRetrieval=true"

      user = "root"
      user = ${?MYSQL_USER}

      password = "iacsocroot"
      password = ${?MYSQL_PASSWORD}

      # Connection Pool settings
      poolInitialSize = 5
      poolMaxSize = 8
      poolConnectionTimeoutMillis = 1000
    }
  presto_analyst
    {
      host = "172.21.32.107"
      host = ${?PRESTO_ANALYST_HOST}

      port = "8080"
      port = ${?PRESTO_ANALYST_PORT}

      catalog = "hive"
      catalog = ${?PRESTO_CATLOG}

      user = "analyst"
      user = ${?PRESTO_ANALYST_USER}

      schema = "default"
      schema = ${?PRESTO_SCHEMA}

      source = "analyst"
      source = ${?PRESTO_ANALYST_SOURCE}


      applicationname = "analyst"

      driver = "io.prestosql.jdbc.PrestoDriver"
      url = "jdbc:presto://"${db.presto_analyst.host}":"${db.presto_analyst.port}"/"${db.presto_analyst.catalog}"/"${db.presto_analyst.schema}"?applicationNamePrefix="${db.presto_analyst.applicationname}

      uri = "http://"${db.presto_analyst.host}":"${db.presto_analyst.port}
      uri = ${?PRESTO_URI}

      # Connection Pool settings
      poolInitialSize = 5
      poolMaxSize = 8
      poolConnectionTimeoutMillis = 1000

    }
  presto_rules
    {
      host = "172.21.31.119"
      host = ${?PRESTO_RULES_HOST}

      port = "8080"
      port = ${?PRESTO_RULES_PORT}

      catalog = "hive"
      catalog = ${?PRESTO_CATLOG}

      user = "analyst"
      user = ${?PRESTO_RULES_USER}

      schema = "default"
      schema = ${?PRESTO_SCHEMA}

      source = "rules"
      source = ${?PRESTO_RULES_SOURCE}

      applicationname = "rules"

      driver = "io.prestosql.jdbc.PrestoDriver"
      url = "jdbc:presto://"${db.presto_rules.host}":"${db.presto_rules.port}"/"${db.presto_rules.catalog}"/"${db.presto_rules.schema}"?applicationNamePrefix="${db.presto_rules.applicationname}

      uri = "http://"${db.presto_rules.host}":"${db.presto_rules.port}
      uri = ${?PRESTO_URI}

      # Connection Pool settings
      poolInitialSize = 5
      poolMaxSize = 8
      poolConnectionTimeoutMillis = 1000

    }

}
ast {

  lower-bound-in-days = "-15"

  default-query-rows-limit = "50000"

}

setup{
  api: true
  api: ${?SETUP_API}
  consumers: true
  consumers: ${?SETUP_CONSUMERS}
  ingestor: false
  ingestor: ${?SETUP_INGESTOR}
  trustar: true
  trustar: ${?SETUP_TRUSTAR}
}

trustar {

  username = ${?TRUSTAR_USERNAME}
  password = ${?TRUSTAR_PASSWORD}
}

environment = "development"  //development, staging, production
environment = ${?ENVIRONMENT}

block-alerts = true
block-alerts = ${?BLOCK_ALERTS}

lumberjack {

  keystorePassword = "PqG4ZNr4svPPc3xhZ6uN"
  keystorePassword = ${?LUMBERJACK_KEYSTORE_PASSWORD}
  excludeSubstringLine = false
  excludeSubstringLine = ${?LUMBERJACK_EXCLUDE_STRING}

}

s3 {

  disabled = false
  disabled = ${?DISABLE_S3_INGESTION}

  minimize-s3-load = true
  minimize-s3-load = ${?MINIMIZE_S3_LOAD}

  accesskey = "accesskey"
  accesskey = ${?AWS_ACCESS_KEY}
  secretkey = "secretkey"
  secretkey = ${?AWS_SECRET_KEY}

  dailbeast-accesskey = "accesskey"
  dailbeast-accesskey = ${?DAILYBEAST_AWS_ACCESS_KEY}
  dailbeast-secretkey = "secretkey"
  dailbeast-secretkey = ${?DAILYBEAST_AWS_SECRET_KEY}

  camel {

    delay = 25
    delay = ${?S3_CAMEL_DELAY}

    maxMessagesPerPoll = 2
    maxMessagesPerPoll = ${?S3_CAMEL_MAX_MESSAGES_PER_POLL}

    concurrentConsumers = 50
    concurrentConsumers = ${?S3_CAMEL_CONCURRENT_CONSUMERS}

  }

}

